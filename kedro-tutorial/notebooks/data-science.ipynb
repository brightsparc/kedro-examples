{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kedro install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging.config\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.core.magic import register_line_magic\n",
    "\n",
    "# Find the project root (./../../../)\n",
    "startup_error = None\n",
    "project_path = Path('..').resolve()\n",
    "\n",
    "@register_line_magic\n",
    "def reload_kedro(path=None, line=None):\n",
    "    \"\"\"\"Line magic which reloads all Kedro default variables.\"\"\"\n",
    "    global startup_error\n",
    "    global context\n",
    "    global catalog\n",
    "\n",
    "    try:\n",
    "        import kedro.config.default_logger\n",
    "        from kedro.context import load_context\n",
    "        from kedro.cli.jupyter import collect_line_magic\n",
    "    except ImportError:\n",
    "        logging.error(\n",
    "            \"Kedro appears not to be installed in your current environment \"\n",
    "            \"or your current IPython session was not started in a valid Kedro project.\"\n",
    "        )\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        path = path or project_path\n",
    "        logging.debug(\"Loading the context from %s\", str(path))\n",
    "\n",
    "        context = load_context(path)\n",
    "        catalog = context.catalog\n",
    "        logging.info(\"** Kedro project %s\", str(context.project_name))\n",
    "        logging.info(\"Defined global variable `context` and `catalog`\")\n",
    "\n",
    "        for line_magic in collect_line_magic():\n",
    "            register_line_magic(line_magic)\n",
    "            logging.info(\"Registered line magic `%s`\", line_magic.__name__)\n",
    "    except Exception as err:\n",
    "        startup_error = err\n",
    "        logging.exception(\n",
    "            \"Kedro's ipython session startup script failed:\\n%s\", str(err)\n",
    "        )\n",
    "        raise err\n",
    "        \n",
    "reload_kedro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = context.catalog.load(\"master_table\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Science pipeline\n",
    "\n",
    "Ssplit the data, then train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the train_test split\n",
    "from kedro_tutorial.pipelines.data_science.nodes import (\n",
    "    evaluate_model,\n",
    "    split_data,\n",
    "    train_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_train, X_test, y_train, y_test] = split_data(df, { 'test_size': 0.2, 'random_state': 3 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(regressor, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "joblib.dump(regressor, os.path.join('', \"model.joblib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Sagemaker SKLearn Estimator \n",
    "\n",
    "<a class=\"anchor\" id=\"train_sklearn\"></a>\n",
    "Training is very simple, just call `fit` on the Estimator! This will start a SageMaker Training job that will download the data for us, invoke our scikit-learn code (in the provided script file), and save any model artifacts that the script creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the script\n",
    "script_path = 'src/kedro_tutorial/pipelines/data_science/nodes.py'\n",
    "\n",
    "!tail -n 50 $script_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we can get to the training data\n",
    "bucket_name = 'kedro-ap-southeast-2-691313291965'\n",
    "master_table_path = 'data/03_primary/master_table.csv'\n",
    "train_input = 's3://{}/{}'.format(bucket_name, master_table_path)\n",
    "\n",
    "!aws s3 cp $train_input .\n",
    "!head -3 master_table.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    role=role,\n",
    "    hyperparameters={'test_size': 0.2, 'random_state': 3 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the trained model to make inference requests <a class=\"anchor\" id=\"inference\"></a>\n",
    "\n",
    "### Deploy the model <a class=\"anchor\" id=\"deploy\"></a>\n",
    "\n",
    "Deploying the model to SageMaker hosting just requires a `deploy` call on the fitted model. This call takes an instance count and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sklearn.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint cleanup <a class=\"anchor\" id=\"endpoint_cleanup\"></a>\n",
    "\n",
    "When you're done with the endpoint, you'll want to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro_test",
   "language": "python",
   "name": "kedro_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
